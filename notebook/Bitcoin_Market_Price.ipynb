{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CnJo7qAT6XDK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "from tensorflow.keras.models import Sequential\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv1D, Flatten\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras.layers import Dense, LSTM\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from keras.regularizers import l1, l2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'BTC-USD3.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "W-zo323s6x4P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ML Model Without Optmization Techniques"
      ],
      "metadata": {
        "id": "_j_WI8isn9Yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data['Date'] = pd.to_datetime(data['Date'])\n",
        "\n",
        "target = data['Close'].values\n",
        "\n",
        "features = data[['Close']].values\n",
        "\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "features_normalized = scaler.fit_transform(features)\n",
        "\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back), 0]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + look_back, 0])\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "# Generate the dataset with a window size (look_back) of 5 days\n",
        "look_back = 5\n",
        "X, y = create_dataset(features_normalized, look_back)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "X_train.shape, X_test.shape\n"
      ],
      "metadata": {
        "id": "u42boU9AfY9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(50, input_shape=(look_back, 1)))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=0)\n",
        "\n",
        "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "test_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIVT509Kfqyy",
        "outputId": "3192bb83-640e-4d9e-f6fc-e6cfd68914b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.08676866441965103"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "plt.plot(train_loss, label='Training Loss')\n",
        "plt.plot(val_loss, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IWVuKeNwgNS1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_prices = model.predict(X_test)\n",
        "\n",
        "# Invert normalization for a true performance measure\n",
        "predicted_prices_inv = scaler.inverse_transform(predicted_prices)\n",
        "actual_prices_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Calculate MSE\n",
        "mse = mean_squared_error(actual_prices_inv, predicted_prices_inv)\n",
        "print(f\"Test MSE: {mse}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-fvoHBjy7D7B",
        "outputId": "6ed168ee-f938-40ec-a6d5-a8c0b5ec01a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7/7 [==============================] - 0s 1ms/step\n",
            "Test MSE: 153018677.42696476\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L1 Regularization"
      ],
      "metadata": {
        "id": "revcUFsMntHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample data for preprocessing\n",
        "look_back = 5\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back), 0]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + look_back, 0])\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "features_normalized = np.random.rand(1000, 1)  # Placeholder for normalized features\n",
        "X, y = create_dataset(features_normalized, look_back)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))"
      ],
      "metadata": {
        "id": "X7qrWehc94WK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(LSTM(50, input_shape=(look_back, 1), kernel_regularizer=l1(0.01)))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=1)\n",
        "\n",
        "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test loss: {test_loss}')"
      ],
      "metadata": {
        "id": "S6zZWWrQkAQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nGwgv4Jyj6TN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# L2 Regularization Model"
      ],
      "metadata": {
        "id": "NYKgb595nQQO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data preprocessing\n",
        "look_back = 5\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back), 0]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + look_back, 0])\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "features_normalized = np.random.rand(1000, 1)\n",
        "X, y = create_dataset(features_normalized, look_back)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(50, input_shape=(look_back, 1), kernel_regularizer=l2(0.01)))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=1)"
      ],
      "metadata": {
        "id": "3f7So2HjiJgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test loss: {test_loss}')"
      ],
      "metadata": {
        "id": "nk2TF84MqKeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BW7ePgfvqDqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Network"
      ],
      "metadata": {
        "id": "D2LRa5YRnFJR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "look_back = 5\n",
        "def create_dataset(dataset, look_back=1):\n",
        "    dataX, dataY = [], []\n",
        "    for i in range(len(dataset)-look_back-1):\n",
        "        a = dataset[i:(i+look_back), 0]\n",
        "        dataX.append(a)\n",
        "        dataY.append(dataset[i + look_back, 0])\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "features_normalized = np.random.rand(1000, 1)\n",
        "X, y = create_dataset(features_normalized, look_back)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv1D(filters=64, kernel_size=2, activation='relu', input_shape=(look_back, 1)))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(50, activation='relu'))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "history = model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=1)"
      ],
      "metadata": {
        "id": "0mb1zTZxidwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model\n",
        "test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(f'Test loss: {test_loss}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bSq6SUk2qX_D",
        "outputId": "57a1ade3-fd31-41d3-c197-d9debfbe4dfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 0.08019901812076569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the training and validation loss\n",
        "plt.plot(history.history['loss'], label='train')\n",
        "plt.plot(history.history['val_loss'], label='validation')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iMuYM3GjqRsg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}